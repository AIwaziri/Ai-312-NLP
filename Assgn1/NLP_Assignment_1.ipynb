{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "\"\"\"\n",
    "NLP First Assignment\n",
    "Done by Lama Alawfi\n",
    "ID Number: 4310345\n",
    "\n",
    "This code performs lemmatization and stemming on a text file using SpaCy and custom Porter stemming rules.\n",
    "It prints the number of changes made during both processes.\n",
    "\"\"\""
   ],
   "metadata": {
    "id": "51bIo1Ddt3sI"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Import required libraries\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "# Load SpaCy's English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ],
   "metadata": {
    "id": "QvCJJpU1t_OM",
    "ExecuteTime": {
     "end_time": "2025-02-25T15:43:07.086467Z",
     "start_time": "2025-02-25T15:43:05.124292Z"
    }
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOSError\u001B[0m                                   Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mre\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# Load SpaCy's English model\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m nlp \u001B[38;5;241m=\u001B[39m \u001B[43mspacy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43men_core_web_sm\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/__init__.py:51\u001B[0m, in \u001B[0;36mload\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mload\u001B[39m(\n\u001B[1;32m     28\u001B[0m     name: Union[\u001B[38;5;28mstr\u001B[39m, Path],\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m     config: Union[Dict[\u001B[38;5;28mstr\u001B[39m, Any], Config] \u001B[38;5;241m=\u001B[39m util\u001B[38;5;241m.\u001B[39mSimpleFrozenDict(),\n\u001B[1;32m     35\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Language:\n\u001B[1;32m     36\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \n\u001B[1;32m     38\u001B[0m \u001B[38;5;124;03m    name (str): Package name or model path.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mutil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mload_model\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m        \u001B[49m\u001B[43mname\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m        \u001B[49m\u001B[43mvocab\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mvocab\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdisable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdisable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     55\u001B[0m \u001B[43m        \u001B[49m\u001B[43menable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43menable\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     56\u001B[0m \u001B[43m        \u001B[49m\u001B[43mexclude\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mexclude\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     57\u001B[0m \u001B[43m        \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mconfig\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/spacy/util.py:472\u001B[0m, in \u001B[0;36mload_model\u001B[0;34m(name, vocab, disable, enable, exclude, config)\u001B[0m\n\u001B[1;32m    470\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m OLD_MODEL_SHORTCUTS:\n\u001B[1;32m    471\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE941\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname, full\u001B[38;5;241m=\u001B[39mOLD_MODEL_SHORTCUTS[name]))  \u001B[38;5;66;03m# type: ignore[index]\u001B[39;00m\n\u001B[0;32m--> 472\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mIOError\u001B[39;00m(Errors\u001B[38;5;241m.\u001B[39mE050\u001B[38;5;241m.\u001B[39mformat(name\u001B[38;5;241m=\u001B[39mname))\n",
      "\u001B[0;31mOSError\u001B[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Load the text file\n",
    "with open('wiki_mountain_def.txt', 'r') as file:\n",
    "    text = file.read()"
   ],
   "metadata": {
    "id": "LJ2Tqz4ouA5N",
    "ExecuteTime": {
     "end_time": "2025-02-25T15:43:15.686417Z",
     "start_time": "2025-02-25T15:43:15.684137Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    \"\"\"\n",
    "    Transform words into their lemma form using SpaCy's built-in function.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be lemmatized.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lemmatized words.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    lemmatized_words = [token.lemma_ for token in doc]\n",
    "    return lemmatized_words\n",
    "\n",
    "\n",
    "lemmatized_words = lemmatize_text(text)\n",
    "\n",
    "# Print lemmatization results after Task 1\n",
    "print(\"Task 1: Lemmatization Completed\")\n",
    "print(\"Lemmatized Words:\")\n",
    "print(lemmatized_words)\n",
    "\n",
    "# Task 2: Count how many words changed due to the lemmatization process\n",
    "original_words = text.split()\n",
    "lemmatization_changes = sum(\n",
    "    1 for orig, lemma in zip(original_words, lemmatized_words) if orig != lemma\n",
    ")\n",
    "\n",
    "# Print the number of changes after lemmatization\n",
    "print(f\"Task 2: Total words changed due to lemmatization: {lemmatization_changes}\\n\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ai9UIkwM2he1",
    "outputId": "35afacf7-27a2-40e3-af1c-9c704a9bb4cb",
    "ExecuteTime": {
     "end_time": "2025-02-25T15:43:17.688519Z",
     "start_time": "2025-02-25T15:43:17.658386Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 16\u001B[0m\n\u001B[1;32m     12\u001B[0m     lemmatized_words \u001B[38;5;241m=\u001B[39m [token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc]\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lemmatized_words\n\u001B[0;32m---> 16\u001B[0m lemmatized_words \u001B[38;5;241m=\u001B[39m \u001B[43mlemmatize_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtext\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Print lemmatization results after Task 1\u001B[39;00m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTask 1: Lemmatization Completed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[4], line 11\u001B[0m, in \u001B[0;36mlemmatize_text\u001B[0;34m(text)\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlemmatize_text\u001B[39m(text):\n\u001B[1;32m      2\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03m    Transform words into their lemma form using SpaCy's built-in function.\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;124;03m        list: A list of lemmatized words.\u001B[39;00m\n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m     doc \u001B[38;5;241m=\u001B[39m \u001B[43mnlp\u001B[49m(text)\n\u001B[1;32m     12\u001B[0m     lemmatized_words \u001B[38;5;241m=\u001B[39m [token\u001B[38;5;241m.\u001B[39mlemma_ \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m doc]\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m lemmatized_words\n",
      "\u001B[0;31mNameError\u001B[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "def custom_porter_stemmer(word):\n",
    "    \"\"\"\n",
    "    Apply custom Porter stemming rules to a word.\n",
    "    Avoid stemming short words with length <= 2.\n",
    "\n",
    "    Args:\n",
    "        word (str): The word to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "        str: The stemmed word.\n",
    "    \"\"\"\n",
    "    if len(word) <= 2:\n",
    "        return word\n",
    "    if word.endswith('sses'):\n",
    "        return word[:-2]  # SSES -> SS\n",
    "    elif word.endswith('ies'):\n",
    "        return word[:-3] + 'i'  # IES -> I\n",
    "    elif word.endswith('ss'):\n",
    "        return word  # SS -> SS (no change)\n",
    "    elif word.endswith('s'):\n",
    "        return word[:-1]  # S -> ''\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    \"\"\"\n",
    "    Transform each word in the text using custom Porter stemming rules.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be stemmed.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of stemmed words.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    stemmed_words = [custom_porter_stemmer(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "\n",
    "stemmed_words = stem_text(text)\n",
    "\n",
    "print(\"Task 3: Stemming Completed\")\n",
    "print(\"Stemmed Words:\")\n",
    "print(stemmed_words)\n",
    "\n",
    "# Task 4: Count how many words changed due to the stemming process\n",
    "stemming_changes = sum(\n",
    "    1 for orig, stem in zip(original_words, stemmed_words) if orig != stem\n",
    ")\n",
    "\n",
    "# Print the number of changes after stemming\n",
    "print(f\"Task 4: Total words changed due to stemming: {stemming_changes}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R4-xILbb3MPh",
    "outputId": "dc59c05e-e0da-420b-83bd-d4bb12b33bbb"
   },
   "execution_count": 18,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Task 3: Stemming Completed\n",
      "Stemmed Words:\n",
      "['There', 'is', 'no', 'universally', 'accepted', 'definition', 'of', 'a', 'mountain.', 'Elevation,', 'volume,', 'relief,', 'steepness,', 'spacing', 'and', 'continuity', 'have', 'been', 'used', 'as', 'criteria', 'for', 'defining', 'a', 'mountain.[4]', 'In', 'the', 'Oxford', 'English', 'Dictionary', 'a', 'mountain', 'is', 'defined', 'as', '\"a', 'natural', 'elevation', 'of', 'the', 'earth', 'surface', 'rising', 'more', 'or', 'less', 'abruptly', 'from', 'the', 'surrounding', 'level', 'and', 'attaining', 'an', 'altitude', 'which,', 'relatively', 'to', 'the', 'adjacent', 'elevation,', 'is', 'impressive', 'or', 'notable.\"[4]', 'Whether', 'a', 'landform', 'is', 'called', 'a', 'mountain', 'may', 'depend', 'on', 'local', 'usage.', 'John', \"Whittow'\", 'Dictionary', 'of', 'Physical', 'Geography[5]', 'state', '\"Some', 'authoriti', 'regard', 'eminence', 'above', '600', 'metre', '(1,969', 'ft)', 'as', 'mountains,', 'those', 'below', 'being', 'referred', 'to', 'as', 'hills.\"', 'In', 'the', 'United', 'Kingdom', 'and', 'the', 'Republic', 'of', 'Ireland,', 'a', 'mountain', 'is', 'usually', 'defined', 'as', 'any', 'summit', 'at', 'least', '2,000', 'feet', '(610', 'm)', 'high,[6]', 'which', 'accord', 'with', 'the', 'official', 'UK', \"government'\", 'definition', 'that', 'a', 'mountain,', 'for', 'the', 'purpose', 'of', 'access,', 'is', 'a', 'summit', 'of', '2,000', 'feet', '(610', 'm)', 'or', 'higher.[7]', 'In', 'addition,', 'some', 'definition', 'also', 'include', 'a', 'topographical', 'prominence', 'requirement,', 'such', 'as', 'that', 'the', 'mountain', 'rise', '300', 'metre', '(984', 'ft)', 'above', 'the', 'surrounding', 'terrain.[1]', 'At', 'one', 'time', 'the', 'US', 'Board', 'on', 'Geographic', 'Name', 'defined', 'a', 'mountain', 'as', 'being', '1,000', 'feet', '(305', 'm)', 'or', 'taller,[8]', 'but', 'ha', 'abandoned', 'the', 'definition', 'since', 'the', '1970s.', 'Any', 'similar', 'landform', 'lower', 'than', 'thi', 'height', 'wa', 'considered', 'a', 'hill.', 'However,', 'today,', 'the', 'US', 'Geological', 'Survey', 'conclude', 'that', 'these', 'term', 'do', 'not', 'have', 'technical', 'definition', 'in', 'the', 'US.[9]']\n",
      "Task 4: Total words changed due to stemming: 18\n"
     ]
    }
   ]
  }
 ]
}
